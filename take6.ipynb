{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import textract\n",
    "import nltk\n",
    "import csv\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "# Set up the global variables used in other methods (word and terms lists)\n",
    "def initializeConstants():\n",
    "    # Stop Words: A stop word is a commonly used word (such as “the”, “a”, “an”, “in”) that a search engine has been programmed to ignore, both when indexing entries for searching and when retrieving them as the result of a search query.\n",
    "    global StopWords\n",
    "    StopWords = set(nltk.corpus.stopwords.words(\"english\"))\n",
    "\n",
    "    # Get the set of all English words (https://stackoverflow.com/a/3788947)\n",
    "    global EnglishWords\n",
    "    with open(\"words.txt\") as word_file:\n",
    "        wordsFile = set(word.strip().lower() for word in word_file)\n",
    "    # Words from the NLTK corpus\n",
    "    nltkWords = set(w.lower() for w in nltk.corpus.words.words())\n",
    "    # A file I made that include things that are words but aren't in the other lists (hacky)\n",
    "    with open(\"words_also.txt\") as word_file:\n",
    "        alsoWords = set(word.strip().lower() for word in word_file)\n",
    "    EnglishWords = wordsFile.union(nltkWords).union(alsoWords)\n",
    "\n",
    "    # Load the terms lists into one big array\n",
    "    global TermsOfInterest\n",
    "    TermsOfInterest = []\n",
    "    for thisFilename in os.listdir(TERMS_LOCATION):\n",
    "        if SKIP_GREEK and thisFilename == \"greek.csv\":\n",
    "            continue\n",
    "        TermsOfInterest = [*TermsOfInterest, *(readCSV(thisFilename))]\n",
    "\n",
    "\n",
    "# Create the files and folders that will organize the intermediate and final output\n",
    "def createFileStructure():\n",
    "    # Uee the date and time to identify the output for convenience\n",
    "    global timestamp\n",
    "    timestamp = t0.strftime(\"%Y-%m-%d__%p-%H_%M_%S\")\n",
    "\n",
    "    # Save the directory path because we'll use it a lot\n",
    "    global THIS_RUN_OUTPUT_DIR\n",
    "    THIS_RUN_OUTPUT_DIR = OUTPUT_DIRECTORY + timestamp + \"/\"\n",
    "\n",
    "    # Make the directories\n",
    "    os.mkdir(THIS_RUN_OUTPUT_DIR)\n",
    "\n",
    "    # Create the word count CSVs\n",
    "    with open(THIS_RUN_OUTPUT_DIR + \"words_headers.csv\", \"w\") as outputFile:\n",
    "        outputFile.write(RESUME_ID_COLUMN_NAME)\n",
    "    with open(THIS_RUN_OUTPUT_DIR + \"bigrams_headers.csv\", \"w\") as outputFile:\n",
    "        outputFile.write(RESUME_ID_COLUMN_NAME)\n",
    "    with open(THIS_RUN_OUTPUT_DIR + \"trigrams_headers.csv\", \"w\") as outputFile:\n",
    "        outputFile.write(RESUME_ID_COLUMN_NAME)\n",
    "\n",
    "    # Terms CSV stuff:\n",
    "    # Each term has a column, using the first term as the general name\n",
    "    headers = [synonyms[0] for synonyms in TermsOfInterest]\n",
    "    # Add a column for the resume ID\n",
    "    headers.insert(0, RESUME_ID_COLUMN_NAME)\n",
    "    # Create the CSV and return its reference\n",
    "    csvfile = open(OUTPUT_DIRECTORY + timestamp + \"/\" + \"terms.csv\", \"w+\")\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=headers)\n",
    "    writer.writeheader()\n",
    "    return writer, csvfile\n",
    "\n",
    "\n",
    "# Read in the terms lists\n",
    "def readCSV(filename):\n",
    "    with open(TERMS_LOCATION + filename, mode=\"r\") as file:\n",
    "        lines = list()\n",
    "        for line in csv.reader(file):\n",
    "            thisLine = []\n",
    "            for term in line:\n",
    "                thisLine.append(term.strip())\n",
    "            lines.append(thisLine)\n",
    "        return lines\n",
    "\n",
    "\n",
    "# Check if the token is a valid English word\n",
    "def isEnglishWord(token):\n",
    "    word = token.lower()\n",
    "\n",
    "    # is itself a word\n",
    "    if word in EnglishWords:\n",
    "        return True\n",
    "\n",
    "    # is made of words separated by punctuation (with no space)\n",
    "    punctuation = [\"-\", \".\", \"/\", \":\"]\n",
    "    for pMark in punctuation:\n",
    "        if pMark in word:\n",
    "            allSubwordsAreWords = True\n",
    "            for subword in word.split(pMark):\n",
    "                if not isEnglishWord(subword):\n",
    "                    allSubwordsAreWords = False\n",
    "                    break\n",
    "            if allSubwordsAreWords:\n",
    "                return True\n",
    "\n",
    "    return False\n",
    "\n",
    "\n",
    "# Naive check: the first character is uppercase and the last isn't\n",
    "def isProperNoun(token):\n",
    "    return token[0].isupper() and token[-1].islower()\n",
    "\n",
    "\n",
    "# Naive check: the token is all uppercase\n",
    "def isAcronym(token):\n",
    "    return token.isupper()\n",
    "\n",
    "\n",
    "# Checks that the token is: (a) a valid English word, (b) a proper noun, or (c) an acronym\n",
    "def isValidTerm(token):\n",
    "    if isEnglishWord(token):\n",
    "        return True\n",
    "\n",
    "    if isProperNoun(token):\n",
    "        return True\n",
    "\n",
    "    if isAcronym(token):\n",
    "        return True\n",
    "\n",
    "    return False\n",
    "\n",
    "\n",
    "# Go through the tokens of a text and get rid of junk\n",
    "def getValidWords(word_tokens):\n",
    "    retVal = []\n",
    "    for token in word_tokens:\n",
    "        # Ignore tokens that begin with non-alphanumeric characters\n",
    "        if token[0].isalpha():\n",
    "            # Ignore stop words\n",
    "            if token.lower() not in StopWords:\n",
    "                # Check validity of token\n",
    "                if isValidTerm(token):\n",
    "                    # Only lower-case it after checks\n",
    "                    retVal.append(token.lower())\n",
    "    return retVal\n",
    "\n",
    "\n",
    "# Find all the occurances of terms of interest in the text\n",
    "def searchForTerms(text, tokens):\n",
    "    wordCounts = countWords(tokens)\n",
    "    termCounts = {}\n",
    "\n",
    "    for term in TermsOfInterest:\n",
    "        rowID = term[0]\n",
    "        termCounts[rowID] = 0\n",
    "\n",
    "        for synonym in term:\n",
    "            # Check for multi-word term (which won't be in tokens)\n",
    "            if len(synonym.split(\" \")) > 1:\n",
    "                termCounts[rowID] += text.count(synonym)\n",
    "            else:\n",
    "                termCounts[rowID] += wordCounts[synonym]\n",
    "\n",
    "    return termCounts\n",
    "\n",
    "\n",
    "# Returns {word: count} for every word in a text\n",
    "def countWords(words):\n",
    "    return Counter(words)\n",
    "\n",
    "# Sort {word: count} pairs by count\n",
    "def sortByCount(words):\n",
    "    return dict(sorted(words.items(), key=lambda item: item[1], reverse=True))\n",
    "\n",
    "\n",
    "# Sort {word: count} pairs by word\n",
    "def sortAlphabetically(words):\n",
    "    return dict(sorted(words.items(), key=lambda item: item[0]))\n",
    "\n",
    "\n",
    "# Sort (by count) and filter the word list\n",
    "def postprocessAllWords(allSeen):\n",
    "    newList = {}\n",
    "    # Could do some filtering here\n",
    "    for term, count in allSeen.items():\n",
    "        if count > 1:\n",
    "            # If term is a tuple (bi/trigrams), JSON will fail -- make string\n",
    "            if type(term) == tuple:\n",
    "                term = \" \".join(term)\n",
    "            newList[term] = count\n",
    "    return sortByCount(newList)\n",
    "\n",
    "initializeConstants()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating output file structure...\n",
      "(took 0.011244 seconds)\n",
      "\n",
      "Searching resumes...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▎         | 93/2538 [00:11<05:03,  8.05it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/smac/Archive/03 Work/Sociology Department/resume-parser/take6.ipynb Cell 2\u001b[0m in \u001b[0;36m<cell line: 39>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/smac/Archive/03%20Work/Sociology%20Department/resume-parser/take6.ipynb#W1sZmlsZQ%3D%3D?line=39'>40</a>\u001b[0m file_path \u001b[39m=\u001b[39m RESUME_DIRECTORY \u001b[39m+\u001b[39m thisFilename\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/smac/Archive/03%20Work/Sociology%20Department/resume-parser/take6.ipynb#W1sZmlsZQ%3D%3D?line=41'>42</a>\u001b[0m \u001b[39m# text -- the plain text of the resume (case-sensitive)\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/smac/Archive/03%20Work/Sociology%20Department/resume-parser/take6.ipynb#W1sZmlsZQ%3D%3D?line=42'>43</a>\u001b[0m text \u001b[39m=\u001b[39m textract\u001b[39m.\u001b[39;49mprocess(file_path)\u001b[39m.\u001b[39mdecode(\u001b[39m\"\u001b[39m\u001b[39mutf8\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/smac/Archive/03%20Work/Sociology%20Department/resume-parser/take6.ipynb#W1sZmlsZQ%3D%3D?line=43'>44</a>\u001b[0m \u001b[39m# wordTokens -- tokens in the text (space-separated, but a bit fancier)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/smac/Archive/03%20Work/Sociology%20Department/resume-parser/take6.ipynb#W1sZmlsZQ%3D%3D?line=44'>45</a>\u001b[0m tokens \u001b[39m=\u001b[39m nltk\u001b[39m.\u001b[39mtokenize\u001b[39m.\u001b[39mword_tokenize(text)\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/resume_parser/lib/python3.10/site-packages/textract/parsers/__init__.py:79\u001b[0m, in \u001b[0;36mprocess\u001b[0;34m(filename, input_encoding, output_encoding, extension, **kwargs)\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[39m# do the extraction\u001b[39;00m\n\u001b[1;32m     78\u001b[0m parser \u001b[39m=\u001b[39m filetype_module\u001b[39m.\u001b[39mParser()\n\u001b[0;32m---> 79\u001b[0m \u001b[39mreturn\u001b[39;00m parser\u001b[39m.\u001b[39;49mprocess(filename, input_encoding, output_encoding, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/resume_parser/lib/python3.10/site-packages/textract/parsers/utils.py:46\u001b[0m, in \u001b[0;36mBaseParser.process\u001b[0;34m(self, filename, input_encoding, output_encoding, **kwargs)\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[39m\"\"\"Process ``filename`` and encode byte-string with ``encoding``. This\u001b[39;00m\n\u001b[1;32m     37\u001b[0m \u001b[39mmethod is called by :func:`textract.parsers.process` and wraps\u001b[39;00m\n\u001b[1;32m     38\u001b[0m \u001b[39mthe :meth:`.BaseParser.extract` method in `a delicious unicode\u001b[39;00m\n\u001b[1;32m     39\u001b[0m \u001b[39msandwich <http://nedbatchelder.com/text/unipain.html>`_.\u001b[39;00m\n\u001b[1;32m     40\u001b[0m \n\u001b[1;32m     41\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     42\u001b[0m \u001b[39m# make a \"unicode sandwich\" to handle dealing with unknown\u001b[39;00m\n\u001b[1;32m     43\u001b[0m \u001b[39m# input byte strings and converting them to a predictable\u001b[39;00m\n\u001b[1;32m     44\u001b[0m \u001b[39m# output encoding\u001b[39;00m\n\u001b[1;32m     45\u001b[0m \u001b[39m# http://nedbatchelder.com/text/unipain/unipain.html#35\u001b[39;00m\n\u001b[0;32m---> 46\u001b[0m byte_string \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mextract(filename, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     47\u001b[0m unicode_string \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdecode(byte_string, input_encoding)\n\u001b[1;32m     48\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mencode(unicode_string, output_encoding)\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/resume_parser/lib/python3.10/site-packages/textract/parsers/pdf_parser.py:21\u001b[0m, in \u001b[0;36mParser.extract\u001b[0;34m(self, filename, method, **kwargs)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[39mif\u001b[39;00m method \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39m'\u001b[39m \u001b[39mor\u001b[39;00m method \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mpdftotext\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m     20\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 21\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mextract_pdftotext(filename, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     22\u001b[0m     \u001b[39mexcept\u001b[39;00m ShellError \u001b[39mas\u001b[39;00m ex:\n\u001b[1;32m     23\u001b[0m         \u001b[39m# If pdftotext isn't installed and the pdftotext method\u001b[39;00m\n\u001b[1;32m     24\u001b[0m         \u001b[39m# wasn't specified, then gracefully fallback to using\u001b[39;00m\n\u001b[1;32m     25\u001b[0m         \u001b[39m# pdfminer instead.\u001b[39;00m\n\u001b[1;32m     26\u001b[0m         \u001b[39mif\u001b[39;00m method \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39m'\u001b[39m \u001b[39mand\u001b[39;00m ex\u001b[39m.\u001b[39mis_not_installed():\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/resume_parser/lib/python3.10/site-packages/textract/parsers/pdf_parser.py:44\u001b[0m, in \u001b[0;36mParser.extract_pdftotext\u001b[0;34m(self, filename, **kwargs)\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     43\u001b[0m     args \u001b[39m=\u001b[39m [\u001b[39m'\u001b[39m\u001b[39mpdftotext\u001b[39m\u001b[39m'\u001b[39m, filename, \u001b[39m'\u001b[39m\u001b[39m-\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[0;32m---> 44\u001b[0m stdout, _ \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrun(args)\n\u001b[1;32m     45\u001b[0m \u001b[39mreturn\u001b[39;00m stdout\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/resume_parser/lib/python3.10/site-packages/textract/parsers/utils.py:102\u001b[0m, in \u001b[0;36mShellParser.run\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m     98\u001b[0m     \u001b[39melse\u001b[39;00m: \u001b[39mraise\u001b[39;00m \u001b[39m#Reraise the last exception unmodified\u001b[39;00m\n\u001b[1;32m    100\u001b[0m \u001b[39m# pipe.wait() ends up hanging on large files. using\u001b[39;00m\n\u001b[1;32m    101\u001b[0m \u001b[39m# pipe.communicate appears to avoid this issue\u001b[39;00m\n\u001b[0;32m--> 102\u001b[0m stdout, stderr \u001b[39m=\u001b[39m pipe\u001b[39m.\u001b[39;49mcommunicate()\n\u001b[1;32m    104\u001b[0m \u001b[39m# if pipe is busted, raise an error (unlike Fabric)\u001b[39;00m\n\u001b[1;32m    105\u001b[0m \u001b[39mif\u001b[39;00m pipe\u001b[39m.\u001b[39mreturncode \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m:\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/resume_parser/lib/python3.10/subprocess.py:1152\u001b[0m, in \u001b[0;36mPopen.communicate\u001b[0;34m(self, input, timeout)\u001b[0m\n\u001b[1;32m   1149\u001b[0m     endtime \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m   1151\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1152\u001b[0m     stdout, stderr \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_communicate(\u001b[39minput\u001b[39;49m, endtime, timeout)\n\u001b[1;32m   1153\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyboardInterrupt\u001b[39;00m:\n\u001b[1;32m   1154\u001b[0m     \u001b[39m# https://bugs.python.org/issue25942\u001b[39;00m\n\u001b[1;32m   1155\u001b[0m     \u001b[39m# See the detailed comment in .wait().\u001b[39;00m\n\u001b[1;32m   1156\u001b[0m     \u001b[39mif\u001b[39;00m timeout \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/resume_parser/lib/python3.10/subprocess.py:2003\u001b[0m, in \u001b[0;36mPopen._communicate\u001b[0;34m(self, input, endtime, orig_timeout)\u001b[0m\n\u001b[1;32m   1996\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_timeout(endtime, orig_timeout,\n\u001b[1;32m   1997\u001b[0m                         stdout, stderr,\n\u001b[1;32m   1998\u001b[0m                         skip_check_and_raise\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m   1999\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(  \u001b[39m# Impossible :)\u001b[39;00m\n\u001b[1;32m   2000\u001b[0m         \u001b[39m'\u001b[39m\u001b[39m_check_timeout(..., skip_check_and_raise=True) \u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m   2001\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mfailed to raise TimeoutExpired.\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m-> 2003\u001b[0m ready \u001b[39m=\u001b[39m selector\u001b[39m.\u001b[39;49mselect(timeout)\n\u001b[1;32m   2004\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_timeout(endtime, orig_timeout, stdout, stderr)\n\u001b[1;32m   2006\u001b[0m \u001b[39m# XXX Rewrite these to use non-blocking I/O on the file\u001b[39;00m\n\u001b[1;32m   2007\u001b[0m \u001b[39m# objects; they are no longer using C stdio!\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/resume_parser/lib/python3.10/selectors.py:416\u001b[0m, in \u001b[0;36m_PollLikeSelector.select\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    414\u001b[0m ready \u001b[39m=\u001b[39m []\n\u001b[1;32m    415\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 416\u001b[0m     fd_event_list \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_selector\u001b[39m.\u001b[39;49mpoll(timeout)\n\u001b[1;32m    417\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mInterruptedError\u001b[39;00m:\n\u001b[1;32m    418\u001b[0m     \u001b[39mreturn\u001b[39;00m ready\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import warnings\n",
    "\n",
    "###################### Config ########################\n",
    "RESUME_DIRECTORY = \"./sample_resumes/\"\n",
    "TERMS_LOCATION = \"./terms_of_interest/\"\n",
    "OUTPUT_DIRECTORY = \"./output/\"\n",
    "RESUME_ID_COLUMN_NAME = \"resumeName\"\n",
    "# This is a lot of terms and may not add much\n",
    "SKIP_GREEK = True\n",
    "######################################################\n",
    "\n",
    "# Write the {word: count} pairs\n",
    "def collectWords(words, whichWords, resumeName):\n",
    "    postProcessed = postprocessAllWords(countWords(words))\n",
    "    postProcessed[RESUME_ID_COLUMN_NAME] = resumeName\n",
    "    \n",
    "    for word in words:\n",
    "        if word not in whichWords:\n",
    "            whichWords.add(word)\n",
    "\n",
    "    return whichWords\n",
    "\n",
    "\n",
    "global t0\n",
    "t0 = datetime.now()\n",
    "\n",
    "print(\"Creating output file structure...\")\n",
    "termsWriter, termsFileRef = createFileStructure()\n",
    "t2 = datetime.now()\n",
    "print(f\"(took {(t2 - t0).total_seconds()} seconds)\\n\")\n",
    "\n",
    "AllWordsSeen = set()\n",
    "AllBigramsSeen = set()\n",
    "AllTrigramsSeen = set()\n",
    "\n",
    "\n",
    "print(\"Searching resumes...\")\n",
    "for thisFilename in tqdm(os.listdir(RESUME_DIRECTORY)):\n",
    "    file_path = RESUME_DIRECTORY + thisFilename\n",
    "\n",
    "    # text -- the plain text of the resume (case-sensitive)\n",
    "    text = textract.process(file_path).decode(\"utf8\")\n",
    "    # wordTokens -- tokens in the text (space-separated, but a bit fancier)\n",
    "    tokens = nltk.tokenize.word_tokenize(text)\n",
    "\n",
    "    # results -- the counts for each term\n",
    "    results = searchForTerms(text, tokens)\n",
    "    # Add the resume ID column\n",
    "    results[RESUME_ID_COLUMN_NAME] = thisFilename\n",
    "    # Write results to the output CSV\n",
    "    termsWriter.writerow(results)\n",
    "\n",
    "    # words -- tokens that are English word or proper noun or acronym\n",
    "    words = getValidWords(tokens)\n",
    "    bigrams = nltk.bigrams(words)\n",
    "    trigrams = nltk.trigrams(words)\n",
    "    # collectWords -- add to the list of what's been seen across all resumes\n",
    "    AllWordsSeen = collectWords(words, AllWordsSeen, thisFilename)\n",
    "    AllBigramsSeen = collectWords(bigrams, AllBigramsSeen, thisFilename)\n",
    "    AllTrigramsSeen = collectWords(trigrams, AllTrigramsSeen, thisFilename)\n",
    "\n",
    "t3 = datetime.now()\n",
    "print(f\"(took {(t3 - t2).total_seconds()} seconds)\\n\")\n",
    "\n",
    "termsFileRef.close()\n",
    "\n",
    "    \n",
    "\n",
    "print(\"Done! :)\")\n",
    "tLast = datetime.now()\n",
    "print(f\"(total runtime: {(tLast - t0).total_seconds()} seconds)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AllWordsSeen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AllWordsSeen.corr()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 ('resume_parser')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f88c6b66e36ed60c88c9e8ba52fc69fc86e0691c5b971920f4e9b2c6aa6c05b2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
